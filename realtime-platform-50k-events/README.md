# AWS Infrastructure for 50K Messages/Second Event Streaming Platform

## Overview

This infrastructure supports a complete real-time event streaming and data processing pipeline capable of handling **up to 50,000 messages per second**. This is a **cost-optimized** version designed for moderate-scale event processing across multiple domains including **E-Commerce** (orders, inventory updates), **Finance** (transactions, market data), **IoT** (sensor telemetry), **Gaming** (player events), **Logistics** (tracking updates), and **Social Media** (user interactions).

The system ingests event data from producers, processes it in real-time using stream processing, and stores aggregated results in an analytics database for querying.

**Architecture:** Producer â†’ Pulsar â†’ Flink â†’ ClickHouse  
**Target Throughput:** 50,000 messages/second  
**Cost Profile:** ~$150-200/month (optimized for moderate workloads)

---

## ğŸ“š Documentation Guide

### Quick Start
ğŸš€ **[Deployment Runbook](./DEPLOYMENT-RUNBOOK.md)** - Complete step-by-step installation guide with exact commands

### Component Details
Detailed technical documentation for each component:

1. ğŸ“Š **[Producer Load Details](./PRODUCER-LOAD-DETAILS.md)**
   - Event data generator (Java/AVRO)
   - 10K unique event sources, 25 fields per message
   - Scalable: 500-1K msg/sec per pod
   - Adaptable for any domain (e-commerce, finance, IoT, etc.)

2. ğŸ“¨ **[Pulsar Load Details](./PULSAR-LOAD-DETAILS.md)**
   - Message broker deployment
   - ZooKeeper, Broker, BookKeeper architecture
   - EBS storage for cost optimization

3. âš¡ **[Flink Load Details](./FLINK-LOAD-DETAILS.md)**
   - Stream processing pipeline
   - **JDBCFlinkConsumer.java** deep dive
   - 1-minute aggregation windows
   - Exactly-once semantics with checkpointing

4. ğŸ—„ï¸ **[ClickHouse Load Details](./CLICKHOUSE-LOAD-DETAILS.md)**
   - Analytics database setup
   - Schema design (benchmark.sensors_local)
   - Query benchmarking and optimization

### Installation Sequence

For first-time deployment, follow these steps in order:

```
1. Infrastructure  â†’ terraform init, plan, apply
2. Pulsar         â†’ ./pulsar-load/deploy.sh
3. ClickHouse     â†’ ./clickhouse-load/00-install-clickhouse.sh
4. Flink          â†’ ./flink-load/deploy.sh
5. Producer       â†’ ./producer-load/deploy.sh
6. Monitoring     â†’ Access Grafana at localhost:3000
```

**ğŸ‘‰ See [DEPLOYMENT-RUNBOOK.md](./DEPLOYMENT-RUNBOOK.md) for detailed instructions**

---

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         AWS EKS Cluster (us-west-2)                     â”‚
â”‚                         bench-low-infra (k8s 1.31)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   PRODUCER     â”‚â”€â”€â”€â”€â–¶â”‚     PULSAR     â”‚â”€â”€â”€â”€â–¶â”‚      FLINK       â”‚  â”‚
â”‚  â”‚   NODES        â”‚     â”‚   MESSAGE      â”‚     â”‚   STREAM         â”‚  â”‚
â”‚  â”‚                â”‚     â”‚   BROKER       â”‚     â”‚   PROCESSING     â”‚  â”‚
â”‚  â”‚ t3.medium      â”‚     â”‚ t3.large       â”‚     â”‚  t3.large        â”‚  â”‚
â”‚  â”‚ 1-3 nodes      â”‚     â”‚ ZK+Broker+BK   â”‚     â”‚  JM + TM         â”‚  â”‚
â”‚  â”‚                â”‚     â”‚ 2-4 nodes      â”‚     â”‚  2-4 nodes       â”‚  â”‚
â”‚  â”‚ Java/AVRO      â”‚     â”‚ EBS Storage    â”‚     â”‚  Aggregation     â”‚  â”‚
â”‚  â”‚ 500-1K msg/sec â”‚     â”‚ Persistent     â”‚     â”‚  1-min windows   â”‚  â”‚
â”‚  â”‚ per pod        â”‚     â”‚ Queue          â”‚     â”‚  Checkpointing   â”‚  â”‚
â”‚  â”‚ Multi-domain   â”‚     â”‚ Cost-optimized â”‚     â”‚  State mgmt      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                            â”‚           â”‚
â”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                         â–¼                                              â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
â”‚                  â”‚   CLICKHOUSE     â”‚                                  â”‚
â”‚                  â”‚   ANALYTICS DB   â”‚                                  â”‚
â”‚                  â”‚                  â”‚                                  â”‚
â”‚                  â”‚  t3.xlarge       â”‚                                  â”‚
â”‚                  â”‚  2-4 nodes       â”‚                                  â”‚
â”‚                  â”‚  EBS Storage     â”‚                                  â”‚
â”‚                  â”‚  Columnar store  â”‚                                  â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Supporting Infrastructure                        â”‚    â”‚
â”‚  â”‚  â€¢ General nodes (t3.small) - System services                â”‚    â”‚
â”‚  â”‚  â€¢ EBS CSI Driver - Persistent volumes                       â”‚    â”‚
â”‚  â”‚  â€¢ VPC with NAT Gateway - Network connectivity               â”‚    â”‚
â”‚  â”‚  â€¢ S3 Bucket - Flink checkpoints & savepoints                â”‚    â”‚
â”‚  â”‚  â€¢ ECR - Container image registry                            â”‚    â”‚
â”‚  â”‚  â€¢ IAM Roles - Service access control                        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Key Differences from 1M Setup

| Aspect | 50K Setup | 1M Setup |
|--------|-----------|----------|
| **Target Throughput** | 50K msg/sec | 250K+ msg/sec |
| **Producer Instances** | t3.medium (1-3 nodes) | c5.4xlarge (3-16 nodes) |
| **Pulsar Brokers** | t3.large (2-4 nodes) | i7i.8xlarge + NVMe (4-8 nodes) |
| **Flink Nodes** | t3.large (2-4 nodes) | c5.4xlarge (3-8 nodes) |
| **ClickHouse** | t3.xlarge (2-4 nodes) | r6id.4xlarge + NVMe (6-12 nodes) |
| **Storage** | EBS gp3 | NVMe SSD + EBS |
| **Monthly Cost** | ~$150-200 | ~$3,500-4,000 |
| **Use Case** | Moderate workloads, dev/test, small-medium scale | Production, high-scale, enterprise |

## Infrastructure Components

### 1. AWS Virtual Private Cloud (VPC)

**CIDR:** `10.1.0.0/16`

**Subnets:**
- **Private Subnets:** `10.1.0.0/20`, `10.1.16.0/20` (4,096 IPs each)
  - All workload pods run in private subnets
- **Public Subnets:** `10.1.101.0/24`, `10.1.102.0/24` (256 IPs each)
  - NAT Gateway and Load Balancers only

**Network Features:**
- **Availability Zones:** 2 AZs for EKS requirements (us-west-2a, us-west-2b)
- **NAT Gateway:** Single NAT gateway for cost optimization (~$32/month)
- **Cost Optimization:** All workload nodes in single AZ to avoid cross-AZ data transfer

### 2. Amazon EKS Cluster

**Name:** `bench-low-infra`  
**Kubernetes Version:** 1.31  
**Node Profile:** Cost-optimized with smaller instances

**Cluster Addons:**
- CoreDNS, kube-proxy, VPC CNI, EBS CSI Driver

### 3. EKS Node Groups

#### Producer Nodes
- **Instance Type:** `t3.medium` (2 vCPU, 4 GiB RAM)
- **Count:** 1-3 nodes (default: 1)
- **Purpose:** Generate event stream data
- **Throughput:** 500-1,000 msg/sec per pod
- **Total Capacity:** Up to 3,000 msg/sec with 3 nodes

#### Pulsar Nodes
- **ZooKeeper:** t3.small (3 nodes)
- **Broker + BookKeeper:** t3.large (2-4 nodes)
- **Storage:** EBS gp3 (cost-effective persistent storage)
- **No NVMe:** Uses EBS only for cost savings

#### Flink Nodes
- **JobManager:** t3.large (1 node)
- **TaskManager:** t3.large (1-2 nodes)
- **Storage:** EBS gp3 for checkpoints

#### ClickHouse Nodes
- **Instance Type:** `t3.xlarge` (4 vCPU, 16 GiB RAM)
- **Count:** 2-4 nodes (default: 2)
- **Storage:** EBS gp3 only
- **Query Performance:** Optimized for moderate query loads

#### General Nodes
- **Instance Type:** `t3.small` (2 vCPU, 2 GiB RAM)
- **Count:** 1-2 nodes
- **Purpose:** System services, operators

## Component Responsibilities

### 1. Producer (producer-load/)
**What it does:**
- Generates realistic event stream data for any domain
- 10,000 unique event sources (scaled down from 100K)
- Rate: 500-1,000 messages/second per pod
- Total capacity: Up to 3,000 msg/sec (3 pods)

**Use Cases:**
- **E-Commerce:** Order events, inventory updates
- **Finance:** Transaction records, payment events
- **IoT:** Sensor telemetry, device status
- **Gaming:** Player events, achievements
- **Logistics:** Shipment tracking

**See:** [PRODUCER-LOAD-DETAILS.md](./PRODUCER-LOAD-DETAILS.md)

### 2. Pulsar (pulsar-load/)
**What it does:**
- Message broker with EBS persistent storage
- Handles 50K+ messages/second
- Cost-optimized with t3 instances

**Components:**
- **ZooKeeper:** Metadata (t3.small Ã— 3)
- **Broker:** Message routing (t3.large Ã— 2-4)
- **BookKeeper:** Storage on EBS (t3.large Ã— 2-4)

**See:** [PULSAR-LOAD-DETAILS.md](./PULSAR-LOAD-DETAILS.md)

### 3. Flink (flink-load/)
**What it does:**
- Stream processing with 1-minute aggregation
- Handles 50K input â†’ ~800-1,000 aggregated records/min
- Exactly-once processing with S3 checkpoints

**Configuration:**
- JobManager: t3.large (1 node)
- TaskManager: t3.large (1-2 nodes)
- Parallelism: 2-4 task slots

**See:** [FLINK-LOAD-DETAILS.md](./FLINK-LOAD-DETAILS.md)

### 4. ClickHouse (clickhouse-load/)
**What it does:**
- Analytics database with EBS storage
- Handles moderate query loads
- 10-20x compression

**Configuration:**
- Instance: t3.xlarge (2-4 nodes)
- Storage: EBS gp3 (200 GB per node)
- Retention: 30 days

**See:** [CLICKHOUSE-LOAD-DETAILS.md](./CLICKHOUSE-LOAD-DETAILS.md)

## Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DETAILED DATA FLOW (50K msg/sec)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. DATA GENERATION (Producer Pods)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ EventDataProducer.java                   â”‚
   â”‚ â€¢ Generates 10K unique event sources     â”‚
   â”‚ â€¢ 25 fields per message                  â”‚
   â”‚ â€¢ AVRO serialization (~200 bytes)       â”‚
   â”‚ â€¢ Rate: 500-1K msg/sec per pod          â”‚
   â”‚ â€¢ Total: Up to 3K msg/sec (3 pods)      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ AVRO binary
                  â–¼
2. MESSAGE QUEUING (Pulsar - EBS Storage)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Topic: persistent://public/default/      â”‚
   â”‚        event-stream-data                 â”‚
   â”‚ â€¢ LZ4 compression                        â”‚
   â”‚ â€¢ EBS persistent storage                 â”‚
   â”‚ â€¢ Retention: 7 days                      â”‚
   â”‚ â€¢ Capacity: 50K msg/sec                  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ Subscription: flink-consumer
                  â–¼
3. STREAM PROCESSING (Flink - t3.large nodes)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ JDBCFlinkConsumer.java                   â”‚
   â”‚ â€¢ AVRO Deserialization                   â”‚
   â”‚ â€¢ keyBy(event_source_id)                 â”‚
   â”‚ â€¢ 1-minute tumbling windows              â”‚
   â”‚ â€¢ Compute avg/min/max                    â”‚
   â”‚ â€¢ 50K msg/sec â†’ ~800-1K agg/min         â”‚
   â”‚ â€¢ S3 checkpoints (1-min interval)        â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ Aggregated records (batch of 1000)
                  â–¼
4. DATA STORAGE (ClickHouse - EBS)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Table: benchmark.sensors_local           â”‚
   â”‚ â€¢ Engine: ReplicatedMergeTree            â”‚
   â”‚ â€¢ Storage: EBS gp3                       â”‚
   â”‚ â€¢ Compression: ~10-20x ratio             â”‚
   â”‚ â€¢ Replication: 2 replicas                â”‚
   â”‚ â€¢ TTL: 30 days                           â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Performance Characteristics

### Throughput
- **Producer:** 500-1,000 msg/sec per pod (up to 3K total)
- **Pulsar:** 50,000 msg/sec capacity
- **Flink:** 50K msg/sec input â†’ ~800-1,000 records/min output
- **ClickHouse:** 1,000+ inserts/sec, 100K+ queries/sec

### Latency
- **Producer â†’ Pulsar:** <20ms (p99)
- **Pulsar â†’ Flink:** <100ms (p99)
- **Flink Processing:** 1-minute window
- **ClickHouse Queries:** <200ms (aggregations)

### Storage
- **Pulsar Retention:** 7 days
- **ClickHouse TTL:** 30 days
- **Flink Checkpoints:** 7 days (S3)
- **Compression:** 10-20x (ClickHouse)

## Cost Breakdown

### Monthly Cost Estimate (SPOT INSTANCES)

| Component | Instance Type | Count | Monthly Cost |
|-----------|---------------|-------|--------------|
| Producer | t3.medium | 1 | ~$15 |
| Pulsar ZK | t3.small | 3 | ~$27 |
| Pulsar Broker+BK | t3.large | 3 | ~$95 |
| Flink JM | t3.large | 1 | ~$30 |
| Flink TM | t3.large | 2 | ~$60 |
| ClickHouse | t3.xlarge | 2 | ~$150 |
| General | t3.small | 1 | ~$9 |
| **Compute Total** | | | **~$386** |
| NAT Gateway | | 1 | ~$32 |
| EBS Storage | gp3 | ~500 GB | ~$40 |
| S3 Storage | Standard | ~50 GB | ~$1 |
| Data Transfer | Out | ~200 GB | ~$20 |
| **Total (On-Demand)** | | | **~$479/month** |
| **Total (Spot - 60% off)** | | | **~$200-250/month** |

**Cost Savings vs 1M Setup:** ~90% lower ($200 vs $3,500/month)

## Deployment Guide

> ğŸ“– **For detailed step-by-step deployment instructions, see [DEPLOYMENT-RUNBOOK.md](./DEPLOYMENT-RUNBOOK.md)**

### Quick Start

```bash
# 1. Deploy infrastructure
terraform init && terraform apply

# 2. Deploy Pulsar
cd pulsar-load && ./deploy.sh

# 3. Deploy ClickHouse
cd ../clickhouse-load && ./00-install-clickhouse.sh && ./00-create-schema-all-replicas.sh

# 4. Deploy Flink
cd ../flink-load && ./deploy.sh && kubectl apply -f flink-job-deployment.yaml

# 5. Deploy Producer
cd ../producer-load && ./deploy.sh

# 6. Access Grafana
kubectl port-forward -n pulsar svc/grafana 3000:3000
# Open: http://localhost:3000 (admin/admin123)
```

## Scaling Guide

### Scale Up (Increase Throughput)

**To 10K msg/sec:**
- Keep current setup, no changes needed

**To 30K msg/sec:**
- Producer: Scale to 3 pods
- Pulsar: Add 1 more broker node
- Flink: Add 1 more TaskManager

**To 50K msg/sec:**
- Producer: 5 pods (t3.medium)
- Pulsar: 4 broker nodes (t3.large)
- Flink: 3 TaskManagers (t3.large)
- ClickHouse: 3 nodes (t3.xlarge)

### Scale Down (Reduce Costs)

**For 5K msg/sec:**
- Producer: 1 pod
- Pulsar: 2 brokers (t3.medium)
- Flink: 1 TaskManager (t3.medium)
- ClickHouse: 1 node (t3.large)
- **Monthly cost:** ~$100-120

## Monitoring

Access Grafana dashboards at `http://localhost:3000` (after port-forward)

**Available Dashboards:**
- **Pulsar:** Broker metrics, topic throughput, storage
- **Flink:** Job metrics, checkpoints, backpressure
- **ClickHouse:** Query performance, table sizes, ingestion rate

**Credentials:** admin / admin123

## Summary

This infrastructure provides a **cost-optimized, moderate-scale** event streaming platform with:

âœ… **Moderate throughput:** Up to 50K messages/second  
âœ… **Low cost:** ~$200-250/month (with spot instances)  
âœ… **Low latency:** End-to-end <2 seconds  
âœ… **Fault tolerance:** Checkpointing, replication  
âœ… **Scalability:** Can scale up to 1M setup if needed  
âœ… **Multi-domain:** E-commerce, finance, IoT, gaming, logistics  
âœ… **Production-ready:** HA, monitoring, backup

---

## ğŸ“– Complete Documentation

### ğŸš€ Getting Started
- **[Deployment Runbook](./DEPLOYMENT-RUNBOOK.md)** - Step-by-step installation (Start here!)

### ğŸ”§ Component Documentation
1. **[Producer Load Details](./PRODUCER-LOAD-DETAILS.md)** - Event generation (500-1K msg/sec per pod)
2. **[Pulsar Load Details](./PULSAR-LOAD-DETAILS.md)** - Message broker (EBS storage)
3. **[Flink Load Details](./FLINK-LOAD-DETAILS.md)** - Stream processing (t3.large nodes)
4. **[ClickHouse Load Details](./CLICKHOUSE-LOAD-DETAILS.md)** - Analytics database (t3.xlarge)

### ğŸ“Š Monitoring & Operations
- Grafana dashboards: http://localhost:3000 (admin/admin123)
- See [Deployment Runbook](./DEPLOYMENT-RUNBOOK.md#step-13-view-metrics-and-dashboards)

---

## Quick Reference

| Task | Command |
|------|---------|
| Deploy infrastructure | `terraform apply` |
| Deploy Pulsar | `cd pulsar-load && ./deploy.sh` |
| Deploy ClickHouse | `cd clickhouse-load && ./00-install-clickhouse.sh` |
| Deploy Flink | `cd flink-load && ./deploy.sh` |
| Deploy Producer | `cd producer-load && ./deploy.sh` |
| Access Grafana | `kubectl port-forward -n pulsar svc/grafana 3000:3000` |
| Scale producer | `kubectl scale deployment event-producer -n iot-pipeline --replicas=3` |
| Check status | `kubectl get pods --all-namespaces` |

---

## Support

For issues:
1. Check [DEPLOYMENT-RUNBOOK.md](./DEPLOYMENT-RUNBOOK.md#troubleshooting)
2. Review component documentation
3. Check pod logs: `kubectl logs <pod-name> -n <namespace>`

## Upgrade Path

To upgrade to the 1M msg/sec setup:
1. See `../realtime-platform-1million-events/` for high-scale configuration
2. Main differences: Larger instances, NVMe storage, more replicas
3. Migration guide available in deployment runbook
