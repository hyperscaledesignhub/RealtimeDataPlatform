# Custom values for Pulsar deployment on EKS with local NVMe SSDs

# Global settings
namespace: pulsar
initialize: true

# Persistence settings
persistence: true
volumes:
  persistence: true
  local_storage: false  # We'll use custom StorageClass

# Anti-affinity to spread pods across nodes
affinity:
  anti_affinity: true
  type: preferredDuringSchedulingIgnoredDuringExecution

# Component configuration
components:
  zookeeper: true
  bookkeeper: true
  broker: true
  proxy: true
  autorecovery: true
  functions: false
  toolset: true
  pulsar_manager: false

# ZooKeeper Configuration
zookeeper:
  replicaCount: 3
  
  # Node selector to place on dedicated nodes
  nodeSelector:
    component: zookeeper
    pulsar-role: zookeeper
    node-type: zookeeper
  
  # Tolerations for node taints (ZooKeeper nodes have no taints)
  tolerations: []
  
  resources:
    requests:
      memory: "1Gi"
      cpu: "500m"
    limits:
      memory: "2Gi"
      cpu: "1"
  
  volumes:
    persistence: true
    data:
      name: data
      size: 20Gi
      storageClassName: "gp3"  # Use AWS gp3 for ZooKeeper

# BookKeeper Configuration
bookkeeper:
  replicaCount: 6
  
  # Node selector to place on BookKeeper nodes
  nodeSelector:
    component: broker-bookie
    pulsar-role: broker-bookie
    node-type: broker-bookie
  
  # Tolerations for node taints
  tolerations:
  - key: "pulsar-broker"
    value: "true"
    effect: "NoSchedule"
  - key: "pulsar-bookkeeper"
    value: "true"
    effect: "NoSchedule"
  
  resources:
    requests:
      memory: "8Gi"
      cpu: "2"
    limits:
      memory: "12Gi"
      cpu: "4"
  
  # Volume configuration for NVMe SSDs
  volumes:
    persistence: true
    useSingleCommonVolume: false
    
    # Journal on EBS gp3
    journal:
      name: journal
      size: 100Gi
      storageClassName: "gp3"
      local_storage: false
    
    # Ledgers on EBS gp3
    ledgers:
      name: ledgers
      size: 300Gi
      storageClassName: "gp3"
      local_storage: false
  
  # BookKeeper configuration
  configData:
    # Increase journal write buffer
    journalMaxSizeMB: "2048"
    journalMaxBackups: "5"
    
    # Optimize for SSDs
    journalSyncData: "false"
    journalAdaptiveGroupWrites: "true"
    journalFlushWhenQueueEmpty: "true"
    
    # Entry log settings
    entryLogSizeLimit: "2147483648"  # 2GB
    entryLogFilePreAllocationEnabled: "true"
    
    # Flush settings optimized for SSDs
    flushInterval: "60000"
    
    # GC settings
    minorCompactionInterval: "3600"
    majorCompactionInterval: "86400"
    isForceGCAllowWhenNoSpace: "true"
    gcWaitTime: "900000"
    
    # Use RocksDB for better performance
    ledgerStorageClass: "org.apache.bookkeeper.bookie.storage.ldb.DbLedgerStorage"
    
    # Stats
    statsProviderClass: "org.apache.bookkeeper.stats.prometheus.PrometheusMetricsProvider"
    
    # JVM settings for heap and direct memory
    BOOKIE_MEM: "-Xms4096m -Xmx8192m"
    BOOKIE_EXTRA_OPTS: "-XX:MaxDirectMemorySize=4096m"


# Broker Configuration
broker:
  replicaCount: 6
  
  # Node selector
  nodeSelector:
    component: broker-bookie
    pulsar-role: broker-bookie
    node-type: broker-bookie
  
  # Tolerations for node taints
  tolerations:
  - key: "pulsar-broker"
    value: "true"
    effect: "NoSchedule"
  - key: "pulsar-bookkeeper"
    value: "true"
    effect: "NoSchedule"
  
  resources:
    requests:
      memory: "8Gi"
      cpu: "1"
    limits:
      memory: "12Gi"
      cpu: "2"
  
  configData:
    # Broker configuration
    brokerDeleteInactiveTopicsEnabled: "false"
    loadBalancerEnabled: "true"
    
    # Message retention (1 day by default)
    defaultRetentionTimeInMinutes: "1440"
    defaultRetentionSizeInMB: "10240"
    
    # Increase limits
    maxConcurrentLookupRequest: "50000"
    maxConcurrentTopicLoadRequest: "50000"
    
    # Performance tuning
    managedLedgerCacheSizeMB: "512"
    managedLedgerDefaultEnsembleSize: "3"
    managedLedgerDefaultWriteQuorum: "2"
    managedLedgerDefaultAckQuorum: "2"
    
    # Stats
    exposeTopicLevelMetricsInPrometheus: "true"
    exposeConsumerLevelMetricsInPrometheus: "false"
    
    # JVM settings for heap and direct memory
    PULSAR_MEM: "-Xms4096m -Xmx8192m"
    PULSAR_EXTRA_OPTS: "-XX:MaxDirectMemorySize=4096m"

# Proxy Configuration
proxy:
  replicaCount: 2
  
  # Node selector
  nodeSelector:
    component: proxy
    pulsar-role: proxy
    node-type: proxy
  
  # Tolerations for node taints (proxy nodes don't have taints, but adding for consistency)
  tolerations: []
  
  resources:
    requests:
      memory: "1Gi"
      cpu: "500m"
    limits:
      memory: "2Gi"
      cpu: "1"
  
  service:
    type: LoadBalancer
    annotations:
      service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
      service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"

# Monitoring
monitoring:
  prometheus: true
  grafana: true
  
  prometheus:
    enabled: false  # We'll use Victoria Metrics from the helm chart
    
  grafana:
    enabled: true
    adminPassword: "admin123"  # Change this!
    service:
      type: LoadBalancer
      annotations:
        service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
    resources:
      requests:
        memory: "2Gi"
        cpu: "500m"
      limits:
        memory: "4Gi"
        cpu: "2"

# Victoria Metrics for monitoring (as configured in the helm chart)
victoria-metrics-k8s-stack:
  enabled: true
  vmagent:
    enabled: true
  grafana:
    enabled: true
    adminPassword: "admin123"  # Change this!
    service:
      type: LoadBalancer
      annotations:
        service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
    persistence:
      enabled: true
      size: 10Gi
      storageClassName: "gp3"
    resources:
      requests:
        memory: "2Gi"
        cpu: "500m"
      limits:
        memory: "4Gi"
        cpu: "2"